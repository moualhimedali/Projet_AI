{"cells":[{"cell_type":"markdown","id":"dc24e58c-ebc6-4a2e-ae14-6d9b2f8a616d","metadata":{"id":"dc24e58c-ebc6-4a2e-ae14-6d9b2f8a616d"},"source":["**TEAM**: Wissem Boujlida, Majdi Bel Hadj Youssef, Aymen Rebhi, Med Ali Moualhi, Amin Bouhamed, Salma Jdidi, Brahim Lasmer"]},{"cell_type":"markdown","id":"234129b2-5902-43dc-8c3f-4ccecc5483e5","metadata":{"id":"234129b2-5902-43dc-8c3f-4ccecc5483e5"},"source":["In this lab, we will be exploring how to preprocess unstructured text for building a a conceptual graph based recommendation system.<br>\n","Text Preprocessing (normalization) is an essential step in natural language processing (NLP) that involves cleaning and transforming unstructured text data to prepare it for your NLP task. It includes tokenization, stemming, lemmatization, punctuation and stop-word removal, part-of-speech tagging and corefrence resolution."]},{"cell_type":"markdown","id":"89e459b1-1d11-4ff0-ba00-c27889e9d617","metadata":{"id":"89e459b1-1d11-4ff0-ba00-c27889e9d617"},"source":["## **Tokenization**"]},{"cell_type":"markdown","id":"2367c8e5-bcbb-4c5b-bb05-b67e9c37cf6a","metadata":{"id":"2367c8e5-bcbb-4c5b-bb05-b67e9c37cf6a"},"source":["Tokenization is the process of breaking down text into individual tokens. a Token could reference a paragraph, a sentence, a word, sub word, or even a character. In this same step, we will also convert each token in the text to lower case."]},{"cell_type":"code","execution_count":null,"id":"a225114f-daa4-40d0-a195-5d9507f41a5d","metadata":{"id":"a225114f-daa4-40d0-a195-5d9507f41a5d","outputId":"9cffb105-1eb9-48b3-8225-3b0755456e0f"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/wissem/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"id":"390fb4cd-8c44-49b9-bcd5-32baaea3f12e","metadata":{"id":"390fb4cd-8c44-49b9-bcd5-32baaea3f12e"},"outputs":[],"source":["def word_tokenizer(text):\n","    tokens_list = word_tokenize(text)\n","    tokens_list = [token.lower() for token in tokens_list]\n","    return tokens_list"]},{"cell_type":"code","execution_count":null,"id":"4b2c7291-5c18-4cd7-a2da-9bc25518dce1","metadata":{"id":"4b2c7291-5c18-4cd7-a2da-9bc25518dce1"},"outputs":[],"source":["text = \"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\""]},{"cell_type":"code","execution_count":null,"id":"84d63108-3efd-4e67-96ff-e40ab2452099","metadata":{"id":"84d63108-3efd-4e67-96ff-e40ab2452099","outputId":"5eab8d59-d07f-47e5-fc8f-7dc52be2b0ab"},"outputs":[{"data":{"text/plain":["['it',\n"," 'would',\n"," 'be',\n"," 'unfair',\n"," 'to',\n"," 'demand',\n"," 'that',\n"," 'people',\n"," 'cease',\n"," 'pirating',\n"," 'files',\n"," 'when',\n"," 'those',\n"," 'same',\n"," 'people',\n"," 'are',\n"," \"n't\",\n"," 'paid',\n"," 'for',\n"," 'their',\n"," 'participation',\n"," 'in',\n"," 'very',\n"," 'lucrative',\n"," 'network',\n"," 'schemes',\n"," '.',\n"," 'ordinary',\n"," 'people',\n"," 'are',\n"," 'relentlessly',\n"," 'spied',\n"," 'on',\n"," ',',\n"," 'and',\n"," 'not',\n"," 'compensated',\n"," 'for',\n"," 'information',\n"," 'taken',\n"," 'from',\n"," 'them',\n"," '.',\n"," 'while',\n"," 'i',\n"," \"'d\",\n"," 'like',\n"," 'to',\n"," 'see',\n"," 'everyone',\n"," 'eventually',\n"," 'pay',\n"," 'for',\n"," 'music',\n"," 'and',\n"," 'the',\n"," 'like',\n"," ',',\n"," 'i',\n"," \"'d\",\n"," 'not',\n"," 'ask',\n"," 'for',\n"," 'it',\n"," 'until',\n"," 'there',\n"," \"'s\",\n"," 'reciprocity',\n"," '.']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tokens_list = word_tokenizer(text)\n","tokens_list"]},{"cell_type":"markdown","id":"f6233096-5196-48b7-89db-82400c2b51e4","metadata":{"id":"f6233096-5196-48b7-89db-82400c2b51e4"},"source":["We can also operate at the level of sentences, using the sentence tokenizer to a split text into individual sentences as follows:"]},{"cell_type":"code","execution_count":null,"id":"1c647bf5-14dd-4435-ab80-0740c47ea385","metadata":{"id":"1c647bf5-14dd-4435-ab80-0740c47ea385"},"outputs":[],"source":["from nltk.tokenize import sent_tokenize"]},{"cell_type":"code","execution_count":null,"id":"3b85eca0-52bb-41ba-8e29-4910a46bb089","metadata":{"id":"3b85eca0-52bb-41ba-8e29-4910a46bb089"},"outputs":[],"source":["def sentence_tokenizer(text):\n","    sentences_list = sent_tokenize(text)\n","    return sentences_list"]},{"cell_type":"code","execution_count":null,"id":"2fa8af39-d332-4622-95c9-0584221b9c63","metadata":{"id":"2fa8af39-d332-4622-95c9-0584221b9c63"},"outputs":[],"source":["text = \"\"\"Project Risk Management includes the processes of conducting risk management planning, identification, analysis,\n","response planning, response implementation, and monitoring risk on a project. The objectives of project risk management\n","are to increase the probability and/or impact of positive risks and to decrease the probability and/or impact of negative\n","risks, in order to optimize the chances of project success.\"\"\""]},{"cell_type":"code","execution_count":null,"id":"aa5eb8a0-c1f6-477b-8434-f544f3e8b07b","metadata":{"id":"aa5eb8a0-c1f6-477b-8434-f544f3e8b07b","outputId":"3e2f94c7-39b6-4eb0-a4fe-d00dc6f037c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Project Risk Management includes the processes of conducting risk management planning, identification, analysis,\\nresponse planning, response implementation, and monitoring risk on a project.', 'The objectives of project risk management\\nare to increase the probability and/or impact of positive risks and to decrease the probability and/or impact of negative\\nrisks, in order to optimize the chances of project success.']\n"]}],"source":["sentences_list = sentence_tokenizer(text)\n","print(sentences_list)"]},{"cell_type":"markdown","id":"d1d7c9fc-f942-4bbd-be26-0cdc7888bf70","metadata":{"id":"d1d7c9fc-f942-4bbd-be26-0cdc7888bf70"},"source":["## **Remove punctuations**"]},{"cell_type":"markdown","id":"91adf3f1-4a70-47fd-9ff5-31325b66b17f","metadata":{"id":"91adf3f1-4a70-47fd-9ff5-31325b66b17f"},"source":["Punctuation marks are marks indicating how a text should be read and, consequently, understood. But whether punctuation marks should be omitted or retained, it is totally up to the NLP task you're performing and the context."]},{"cell_type":"code","execution_count":null,"id":"0f60c8c4-38fd-4b68-b5e3-a85d18261752","metadata":{"id":"0f60c8c4-38fd-4b68-b5e3-a85d18261752"},"outputs":[],"source":["import string                              # for string operations"]},{"cell_type":"code","execution_count":null,"id":"a2333550-b769-4fd4-9f16-27dec50e2c43","metadata":{"id":"a2333550-b769-4fd4-9f16-27dec50e2c43","outputId":"90c95b1f-6451-4af0-aa2e-19d855d15f3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Punctuation\n","\n","!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"]}],"source":["print('\\nPunctuation\\n')\n","print(string.punctuation)"]},{"cell_type":"code","execution_count":null,"id":"50d329d9-4058-47d5-9c9e-6a809d06c67a","metadata":{"id":"50d329d9-4058-47d5-9c9e-6a809d06c67a"},"outputs":[],"source":["def remove_punctuation(tokens_list):\n","    tokens_clean = []\n","    for token in tokens_list: # Go through every token in your tokens list\n","        if (token not in string.punctuation):  # remove punctuation\n","            tokens_clean.append(token)\n","    return tokens_clean"]},{"cell_type":"code","execution_count":null,"id":"0c995d2a-7d31-4352-bac3-ac9ee3e1be42","metadata":{"id":"0c995d2a-7d31-4352-bac3-ac9ee3e1be42"},"outputs":[],"source":["text = \"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\""]},{"cell_type":"code","execution_count":null,"id":"66adf6a7-0d94-445b-b077-b0177ad7f791","metadata":{"id":"66adf6a7-0d94-445b-b077-b0177ad7f791","outputId":"d6b11578-4982-4b90-8cb3-0de6dc89c8eb"},"outputs":[{"data":{"text/plain":["['it',\n"," 'would',\n"," 'be',\n"," 'unfair',\n"," 'to',\n"," 'demand',\n"," 'that',\n"," 'people',\n"," 'cease',\n"," 'pirating',\n"," 'files',\n"," 'when',\n"," 'those',\n"," 'same',\n"," 'people',\n"," 'are',\n"," \"n't\",\n"," 'paid',\n"," 'for',\n"," 'their',\n"," 'participation',\n"," 'in',\n"," 'very',\n"," 'lucrative',\n"," 'network',\n"," 'schemes',\n"," 'ordinary',\n"," 'people',\n"," 'are',\n"," 'relentlessly',\n"," 'spied',\n"," 'on',\n"," 'and',\n"," 'not',\n"," 'compensated',\n"," 'for',\n"," 'information',\n"," 'taken',\n"," 'from',\n"," 'them',\n"," 'while',\n"," 'i',\n"," \"'d\",\n"," 'like',\n"," 'to',\n"," 'see',\n"," 'everyone',\n"," 'eventually',\n"," 'pay',\n"," 'for',\n"," 'music',\n"," 'and',\n"," 'the',\n"," 'like',\n"," 'i',\n"," \"'d\",\n"," 'not',\n"," 'ask',\n"," 'for',\n"," 'it',\n"," 'until',\n"," 'there',\n"," \"'s\",\n"," 'reciprocity']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["tokens_list = word_tokenizer(text)\n","tokens_clean = remove_punctuation(tokens_list)\n","tokens_clean"]},{"cell_type":"markdown","id":"ee864508-4a36-4e27-b0f4-861c10fee8a4","metadata":{"id":"ee864508-4a36-4e27-b0f4-861c10fee8a4"},"source":["## **Remove Stop Words**"]},{"cell_type":"markdown","id":"0537fb66-a704-4f31-9ff4-d7b66f1cfbc7","metadata":{"id":"0537fb66-a704-4f31-9ff4-d7b66f1cfbc7"},"source":["Stop words are a set of commonly used words that don't add significant meaning to the text. But whether stop words should be omitted or retained, it is totally up to the NLP task you're performing and the context."]},{"cell_type":"code","execution_count":null,"id":"5b011917-d28b-4c91-9da0-947a9fbbfa48","metadata":{"id":"5b011917-d28b-4c91-9da0-947a9fbbfa48","outputId":"08ab24dc-3b11-4e34-fb64-b928a2507bd4"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /home/wissem/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.corpus import stopwords          # module for stop words that come with NLTK\n","# download the stopwords from NLTK\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"id":"ae597afa-3e24-4fb2-8fba-409322d98db0","metadata":{"id":"ae597afa-3e24-4fb2-8fba-409322d98db0","outputId":"bb94386a-a21f-4cd9-fd2e-2f6dfc8d0011"},"outputs":[{"name":"stdout","output_type":"stream","text":["Stop words\n","\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}],"source":["#Import the english stop words list from NLTK\n","stopwords_english = stopwords.words('english')\n","\n","print('Stop words\\n')\n","print(stopwords_english)"]},{"cell_type":"code","execution_count":null,"id":"bbd6d651-b914-482b-afd7-db96a3d09dbf","metadata":{"id":"bbd6d651-b914-482b-afd7-db96a3d09dbf"},"outputs":[],"source":["def remove_stopwords(tokens_list):\n","    #Import the english stop words list from NLTK\n","    stopwords_english = stopwords.words('english')\n","    tokens_clean = []\n","    for token in tokens_list: # Go through every token in your tokens list\n","        if (token not in stopwords_english):  # remove stopwords\n","            tokens_clean.append(token)\n","    return tokens_clean"]},{"cell_type":"code","execution_count":null,"id":"42e2be6b-6836-425a-a1e5-3129114febf1","metadata":{"id":"42e2be6b-6836-425a-a1e5-3129114febf1"},"outputs":[],"source":["text = \"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\""]},{"cell_type":"code","execution_count":null,"id":"28c91895-c41b-445c-9470-8787a4f2fc26","metadata":{"id":"28c91895-c41b-445c-9470-8787a4f2fc26","outputId":"d040baf4-a047-4414-f6a2-7c5868cc1b22"},"outputs":[{"data":{"text/plain":["['would',\n"," 'unfair',\n"," 'demand',\n"," 'people',\n"," 'cease',\n"," 'pirating',\n"," 'files',\n"," 'people',\n"," \"n't\",\n"," 'paid',\n"," 'participation',\n"," 'lucrative',\n"," 'network',\n"," 'schemes',\n"," '.',\n"," 'ordinary',\n"," 'people',\n"," 'relentlessly',\n"," 'spied',\n"," ',',\n"," 'compensated',\n"," 'information',\n"," 'taken',\n"," '.',\n"," \"'d\",\n"," 'like',\n"," 'see',\n"," 'everyone',\n"," 'eventually',\n"," 'pay',\n"," 'music',\n"," 'like',\n"," ',',\n"," \"'d\",\n"," 'ask',\n"," \"'s\",\n"," 'reciprocity',\n"," '.']"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["tokens_list = word_tokenizer(text)\n","tokens_clean = remove_stopwords(tokens_list)\n","tokens_clean"]},{"cell_type":"markdown","id":"20767501-2d25-4b3f-b918-dc7ad95abd0f","metadata":{"id":"20767501-2d25-4b3f-b918-dc7ad95abd0f"},"source":["## **Stemming**"]},{"cell_type":"markdown","id":"e6eff2c5-6d86-4319-9b54-c7ebf05e33e1","metadata":{"id":"e6eff2c5-6d86-4319-9b54-c7ebf05e33e1"},"source":["Stemming is the process of reducing a word to its most general form, or stem(root). This helps in reducing the size of our vocabulary.\n","\n","Consider the words:\n","\n","    learn\n","    learning\n","    learned\n","    learnt\n","\n","All these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not spelled correctly. For example, for happy and sunny. We can look at the set of words that comprises the different forms of happy:\n","\n","    happy\n","    happiness\n","    happier\n","\n","We can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen. So, we choose the most common stem for related words.\n","\n","NLTK has different modules for stemming and we will be using the PorterStemmer module which uses the Porter Stemming Algorithm. We can also use the Snowball Stemmer which is an improvement to the Porter Stemmer, stemming words to a more accurate stem."]},{"cell_type":"code","execution_count":null,"id":"461fd5db-40e5-4558-95d0-7c8b65a95657","metadata":{"id":"461fd5db-40e5-4558-95d0-7c8b65a95657"},"outputs":[],"source":["from nltk.stem import PorterStemmer        # module for stemming"]},{"cell_type":"code","execution_count":null,"id":"e666c765-f3bf-423c-a256-622881418c90","metadata":{"id":"e666c765-f3bf-423c-a256-622881418c90"},"outputs":[],"source":["def stemming(tokens_list):\n","    # Instantiate stemming class\n","    stemmer = PorterStemmer()\n","    stems_list = []\n","    for token in tokens_list: # Go through every token in your tokens list\n","        stem_token = stemmer.stem(token)  # stemming token\n","        stems_list.append(stem_token)  # append to the list of stems\n","    return stems_list"]},{"cell_type":"code","execution_count":null,"id":"5de2621f-ff46-4b3a-85e3-565eeaf3684e","metadata":{"id":"5de2621f-ff46-4b3a-85e3-565eeaf3684e"},"outputs":[],"source":["text = \"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\""]},{"cell_type":"code","execution_count":null,"id":"057bb63a-b032-44c1-9ffe-d9e0139f18ff","metadata":{"id":"057bb63a-b032-44c1-9ffe-d9e0139f18ff","outputId":"d0316e2f-ae84-429d-931f-00c27df01ef1"},"outputs":[{"data":{"text/plain":["['it',\n"," 'would',\n"," 'be',\n"," 'unfair',\n"," 'to',\n"," 'demand',\n"," 'that',\n"," 'peopl',\n"," 'ceas',\n"," 'pirat',\n"," 'file',\n"," 'when',\n"," 'those',\n"," 'same',\n"," 'peopl',\n"," 'are',\n"," \"n't\",\n"," 'paid',\n"," 'for',\n"," 'their',\n"," 'particip',\n"," 'in',\n"," 'veri',\n"," 'lucr',\n"," 'network',\n"," 'scheme',\n"," '.',\n"," 'ordinari',\n"," 'peopl',\n"," 'are',\n"," 'relentlessli',\n"," 'spi',\n"," 'on',\n"," ',',\n"," 'and',\n"," 'not',\n"," 'compens',\n"," 'for',\n"," 'inform',\n"," 'taken',\n"," 'from',\n"," 'them',\n"," '.',\n"," 'while',\n"," 'i',\n"," \"'d\",\n"," 'like',\n"," 'to',\n"," 'see',\n"," 'everyon',\n"," 'eventu',\n"," 'pay',\n"," 'for',\n"," 'music',\n"," 'and',\n"," 'the',\n"," 'like',\n"," ',',\n"," 'i',\n"," \"'d\",\n"," 'not',\n"," 'ask',\n"," 'for',\n"," 'it',\n"," 'until',\n"," 'there',\n"," \"'s\",\n"," 'reciproc',\n"," '.']"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["tokens_list = word_tokenizer(text)\n","stems_list = stemming(tokens_list)\n","stems_list"]},{"cell_type":"code","execution_count":null,"id":"d9ab4180-24d1-4001-891c-e4c2d4a830ed","metadata":{"id":"d9ab4180-24d1-4001-891c-e4c2d4a830ed"},"outputs":[],"source":["from nltk.stem import SnowballStemmer"]},{"cell_type":"code","execution_count":null,"id":"52253969-ccc3-4a39-ba71-84ab3267ef2c","metadata":{"id":"52253969-ccc3-4a39-ba71-84ab3267ef2c"},"outputs":[],"source":["def stemming2(tokens_list):\n","    # Instantiate stemming class\n","    stemmer = SnowballStemmer(\"english\")\n","    stems_list = []\n","    for token in tokens_list: # Go through every token in your tokens list\n","        stem_token = stemmer.stem(token)  # stemming token\n","        stems_list.append(stem_token)  # append to the list of stems\n","    return stems_list"]},{"cell_type":"code","execution_count":null,"id":"38c97784-4b04-4bee-99aa-a7dc65139855","metadata":{"id":"38c97784-4b04-4bee-99aa-a7dc65139855","outputId":"ef3ccfff-5a36-4b8a-92a1-1db2f9d78684"},"outputs":[{"data":{"text/plain":["['it',\n"," 'would',\n"," 'be',\n"," 'unfair',\n"," 'to',\n"," 'demand',\n"," 'that',\n"," 'peopl',\n"," 'ceas',\n"," 'pirat',\n"," 'file',\n"," 'when',\n"," 'those',\n"," 'same',\n"," 'peopl',\n"," 'are',\n"," \"n't\",\n"," 'paid',\n"," 'for',\n"," 'their',\n"," 'particip',\n"," 'in',\n"," 'veri',\n"," 'lucrat',\n"," 'network',\n"," 'scheme',\n"," '.',\n"," 'ordinari',\n"," 'peopl',\n"," 'are',\n"," 'relentless',\n"," 'spi',\n"," 'on',\n"," ',',\n"," 'and',\n"," 'not',\n"," 'compens',\n"," 'for',\n"," 'inform',\n"," 'taken',\n"," 'from',\n"," 'them',\n"," '.',\n"," 'while',\n"," 'i',\n"," \"'d\",\n"," 'like',\n"," 'to',\n"," 'see',\n"," 'everyon',\n"," 'eventu',\n"," 'pay',\n"," 'for',\n"," 'music',\n"," 'and',\n"," 'the',\n"," 'like',\n"," ',',\n"," 'i',\n"," \"'d\",\n"," 'not',\n"," 'ask',\n"," 'for',\n"," 'it',\n"," 'until',\n"," 'there',\n"," \"'s\",\n"," 'reciproc',\n"," '.']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["tokens_list = word_tokenizer(text)\n","stems_list = stemming2(tokens_list)\n","stems_list"]},{"cell_type":"markdown","id":"765f5b27-594e-43cf-88a2-7f96e427de89","metadata":{"id":"765f5b27-594e-43cf-88a2-7f96e427de89"},"source":["## **Part-of-speech tagging (POST)**"]},{"cell_type":"markdown","id":"fe32c9e8-c247-45ce-a1c2-be456312db76","metadata":{"id":"fe32c9e8-c247-45ce-a1c2-be456312db76"},"source":["Part-of-speech tagging (POST), is the process of marking up a word in a text (corpus) to its corresponding part of speech (nouns, verbs, adjectives, adverbs, etc...) based on both its definition and its context.\n","We want the computer to understand the meaning of text correctly and part of that is POST.\n","POST has a lot of applications:\n","    \n","    Text modeling\n","    Autocomplete (figure out the next word)\n","    word ambiguity resolution ( Pick a word like watch and figure out if we're talking about it as a noun or as a verb..)\n","\n","The nltk.tag.AveragedPerceptronTagger is the default tagger as of NLTK version 3.1. This model was trained on on Sections 00-18 of the Wall Street Journal sections of OntoNotes 5. The original implementation comes from Matthew Honnibal, and it outperforms the predecessor maximum entropy POS model in NLTK.\n","You can check this link for the tags set : https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"]},{"cell_type":"code","execution_count":null,"id":"7a80fe3b-7ea7-43f1-a5ec-49d4eaab0a54","metadata":{"id":"7a80fe3b-7ea7-43f1-a5ec-49d4eaab0a54","outputId":"082948a1-dc44-4d64-8730-c520a928222f"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /home/wissem/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package universal_tagset to\n","[nltk_data]     /home/wissem/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tag import pos_tag\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('universal_tagset')"]},{"cell_type":"code","execution_count":null,"id":"236dfb2a-9ee3-4757-a77e-81f5a436c09a","metadata":{"id":"236dfb2a-9ee3-4757-a77e-81f5a436c09a"},"outputs":[],"source":["def POST(tokens_list):\n","    pos_tags = pos_tag(tokens_list, lang='eng')\n","    return pos_tags"]},{"cell_type":"code","execution_count":null,"id":"b4230f24-960a-424f-b144-ae4ec31ad0d9","metadata":{"id":"b4230f24-960a-424f-b144-ae4ec31ad0d9"},"outputs":[],"source":["text = \"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\""]},{"cell_type":"code","execution_count":null,"id":"d6d30e80-1c4b-4f6d-9640-d5ba7657f8a5","metadata":{"id":"d6d30e80-1c4b-4f6d-9640-d5ba7657f8a5","outputId":"0235ee16-88c1-40db-a80f-750e0b37efa6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('it', 'PRP'), ('would', 'MD'), ('be', 'VB'), ('unfair', 'JJ'), ('to', 'TO'), ('demand', 'VB'), ('that', 'IN'), ('people', 'NNS'), ('cease', 'VBP'), ('pirating', 'VBG'), ('files', 'NNS'), ('when', 'WRB'), ('those', 'DT'), ('same', 'JJ'), ('people', 'NNS'), ('are', 'VBP'), (\"n't\", 'RB'), ('paid', 'VBN'), ('for', 'IN'), ('their', 'PRP$'), ('participation', 'NN'), ('in', 'IN'), ('very', 'RB'), ('lucrative', 'JJ'), ('network', 'NN'), ('schemes', 'NNS'), ('.', '.'), ('ordinary', 'JJ'), ('people', 'NNS'), ('are', 'VBP'), ('relentlessly', 'RB'), ('spied', 'VBN'), ('on', 'IN'), (',', ','), ('and', 'CC'), ('not', 'RB'), ('compensated', 'VBN'), ('for', 'IN'), ('information', 'NN'), ('taken', 'VBN'), ('from', 'IN'), ('them', 'PRP'), ('.', '.'), ('while', 'IN'), ('i', 'JJ'), (\"'d\", 'MD'), ('like', 'VB'), ('to', 'TO'), ('see', 'VB'), ('everyone', 'NN'), ('eventually', 'RB'), ('pay', 'VB'), ('for', 'IN'), ('music', 'NN'), ('and', 'CC'), ('the', 'DT'), ('like', 'JJ'), (',', ','), ('i', 'JJ'), (\"'d\", 'MD'), ('not', 'RB'), ('ask', 'VB'), ('for', 'IN'), ('it', 'PRP'), ('until', 'IN'), ('there', 'EX'), (\"'s\", 'VBZ'), ('reciprocity', 'NN'), ('.', '.')]\n"]}],"source":["tokens_list = word_tokenizer(text)\n","pos_tags = POST(tokens_list)\n","print(pos_tags)"]},{"cell_type":"markdown","id":"18327868-8e00-4e06-9258-b6fabdd8bae6","metadata":{"id":"18327868-8e00-4e06-9258-b6fabdd8bae6"},"source":["## **Lemmatization**"]},{"cell_type":"markdown","id":"d150832f-b235-42d5-8dfd-f549aaaaca49","metadata":{"id":"d150832f-b235-42d5-8dfd-f549aaaaca49"},"source":["Lemmatization is also the process of reducing a word to its most base form, or lemma. This helps in reducing the size of our vocabulary.\n","Unlike stemming, lemmatization reduces words to their base word ensuring that the root word belongs to the language. It’s usually more sophisticated than stemming, since stemmers works on an individual word without knowledge of the context. In the other side lemmatizers uses the linguistic knowledge and the context to derive a properly spelled and grammatically correct base form.\n","\n","Just like for stemming, there are different lemmatizers. For this example, we’ll use WordNet lemmatizer."]},{"cell_type":"code","execution_count":null,"id":"f55f11b2-22f3-4c20-aa7a-8bef97af3bdf","metadata":{"id":"f55f11b2-22f3-4c20-aa7a-8bef97af3bdf","outputId":"19f873a4-c99e-44b7-93cd-8a6f0b5229c6"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /home/wissem/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.stem.wordnet import WordNetLemmatizer\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"id":"c5f276b6-18a6-4668-8442-e65190b6b6cd","metadata":{"id":"c5f276b6-18a6-4668-8442-e65190b6b6cd"},"outputs":[],"source":["def lemmatization(tokens_list):\n","    # Instantiate lemmatization class\n","    lemmatizer = WordNetLemmatizer()\n","    lemmas_list = []\n","    for token in tokens_list: # Go through every token in your tokens list\n","        lemma_token = lemmatizer.lemmatize(token)  # stemming token\n","        lemmas_list.append(lemma_token)  # append to the list of stems\n","    return lemmas_list"]},{"cell_type":"code","execution_count":null,"id":"3ed8a66e-85ae-426e-ac2f-655f3fdcb845","metadata":{"id":"3ed8a66e-85ae-426e-ac2f-655f3fdcb845"},"outputs":[],"source":["text = \"the cat is sitting with the bats on the striped mat under many badly flying geese\""]},{"cell_type":"code","execution_count":null,"id":"f744f11f-fd40-41bb-bee2-ffb5298ef5f0","metadata":{"id":"f744f11f-fd40-41bb-bee2-ffb5298ef5f0","outputId":"988e1313-18b2-44ad-a447-46f66e1b548b"},"outputs":[{"data":{"text/plain":["['the',\n"," 'cat',\n"," 'is',\n"," 'sitting',\n"," 'with',\n"," 'the',\n"," 'bat',\n"," 'on',\n"," 'the',\n"," 'striped',\n"," 'mat',\n"," 'under',\n"," 'many',\n"," 'badly',\n"," 'flying',\n"," 'goose']"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["tokens_list = word_tokenizer(text)\n","lemmas_list = lemmatization(tokens_list)\n","lemmas_list"]},{"cell_type":"markdown","id":"098a4b6f-fdf1-4e73-b635-5021d081cb9e","metadata":{"id":"098a4b6f-fdf1-4e73-b635-5021d081cb9e"},"source":["POS tagging can improve lemmatization accuracy. For example, the word ‘leaves’ without a POS tag would get lemmatized to the word ‘leaf’, but with a verb tag, its lemma would become ‘leave’.\n","To get the best results, you’ll have to feed the POS tags to the lemmatizer, or otherwise it might not reduce all the words to the lemmas you desire."]},{"cell_type":"code","execution_count":null,"id":"2e20904c-9c50-4a53-a1d8-ad7141002f85","metadata":{"id":"2e20904c-9c50-4a53-a1d8-ad7141002f85"},"outputs":[],"source":["from nltk.corpus import wordnet"]},{"cell_type":"code","execution_count":null,"id":"6bd00d52-7fe0-49c9-9c57-d63981d34a85","metadata":{"id":"6bd00d52-7fe0-49c9-9c57-d63981d34a85"},"outputs":[],"source":["# Define function to lemmatize each token with its POS tag\n","\n","def lemmatization2(tokens_list):\n","    lemmatizer = WordNetLemmatizer()\n","    nltk_pos_tags = POST(tokens_list)\n","    lemmas_list = []\n","    for nltk_pos_tag in nltk_pos_tags:\n","        token, pos_tag = nltk_pos_tag\n","        if pos_tag.startswith('J'):\n","            lemma = lemmatizer.lemmatize(token, wordnet.ADJ)\n","            lemmas_list.append(lemma)\n","        elif pos_tag.startswith('V'):\n","            lemma = lemmatizer.lemmatize(token, wordnet.VERB)\n","            lemmas_list.append(lemma)\n","        elif pos_tag.startswith('N'):\n","            lemma = lemmatizer.lemmatize(token, wordnet.NOUN)\n","            lemmas_list.append(lemma)\n","        elif pos_tag.startswith('R'):\n","            lemma = lemmatizer.lemmatize(token, wordnet.ADV)\n","            lemmas_list.append(lemma)\n","        else:\n","            lemma = lemmatizer.lemmatize(token)\n","            lemmas_list.append(lemma)\n","    return lemmas_list"]},{"cell_type":"code","execution_count":null,"id":"f0b7386a-1049-46c8-a08b-43ae1dc03c8e","metadata":{"id":"f0b7386a-1049-46c8-a08b-43ae1dc03c8e"},"outputs":[],"source":["text = 'the cat is sitting with the bats on the striped mat under many badly flying geese'"]},{"cell_type":"code","execution_count":null,"id":"6c5998f0-4ff0-42a7-a4d0-80251005d7c7","metadata":{"id":"6c5998f0-4ff0-42a7-a4d0-80251005d7c7","outputId":"ca1a5b67-14a3-4f5d-89cf-7bafc773d23c"},"outputs":[{"data":{"text/plain":["['the',\n"," 'cat',\n"," 'be',\n"," 'sit',\n"," 'with',\n"," 'the',\n"," 'bat',\n"," 'on',\n"," 'the',\n"," 'striped',\n"," 'mat',\n"," 'under',\n"," 'many',\n"," 'badly',\n"," 'fly',\n"," 'geese']"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["tokens_list = word_tokenizer(text)\n","lemmas_list = lemmatization2(tokens_list)\n","lemmas_list"]},{"cell_type":"markdown","id":"2541458e-3da0-47c5-9265-1f5bc08ae556","metadata":{"id":"2541458e-3da0-47c5-9265-1f5bc08ae556"},"source":["## **Coreference resolution**"]},{"cell_type":"markdown","id":"088771a0-11bc-47fd-b75f-b1e4228b4d0a","metadata":{"id":"088771a0-11bc-47fd-b75f-b1e4228b4d0a"},"source":["In linguistics, coreference occurs when two or more expressions refer to the same entity or thing.\n","The coreference resolution converts those expressions into the referred entities. Most often, coreferences are pronouns (He, she, it , my, his...)<br>\n","For example, given the sentence, “John went to the store. He bought some groceries.”, The coreference resolution model would identify that “John” and “He” both refer to the same entity \"John\" and replace the pronoun \"He\" with \"John\". and so the resolved sentence should be: \"John went to the store. John bought some groceries.\""]},{"cell_type":"markdown","id":"88c0c67a-980d-4cd9-ba83-2640d974e2bb","metadata":{"id":"88c0c67a-980d-4cd9-ba83-2640d974e2bb"},"source":["\n","we will be using the new Crosslingual Coreference model contributed by David Berenstein to the spaCy Universe."]},{"cell_type":"code","execution_count":null,"id":"7802363a-75d4-4624-ba98-6b7bcfba15f6","metadata":{"id":"7802363a-75d4-4624-ba98-6b7bcfba15f6","outputId":"6cc4c51b-d93d-418c-d3b6-311043d4d2f5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/wissem/workspace/projects/graph_recommendation_system/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import spacy"]},{"cell_type":"code","execution_count":null,"id":"61c0e476-cc3d-4116-a539-4c398cbd796a","metadata":{"id":"61c0e476-cc3d-4116-a539-4c398cbd796a"},"outputs":[],"source":["def coreference_resolution(text):\n","    # use any model that has internal spacy embeddings\n","    DEVICE = -1 # Number of the GPU, -1 if want to use CPU\n","    coreference_resolution_model = spacy.load('en_core_web_sm')\n","    coreference_resolution_model.add_pipe(\n","        \"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": DEVICE})\n","    resolved_text = coreference_resolution_model(text)._.resolved_text\n","    return resolved_text"]},{"cell_type":"code","execution_count":null,"id":"3219e82b-cd74-4414-b4f2-67633daf61c5","metadata":{"id":"3219e82b-cd74-4414-b4f2-67633daf61c5"},"outputs":[],"source":["text = \"\"\"\n","    Do not forget about Momofuku Ando!\n","    He created instant noodles in Osaka.\n","    At that location, Nissin was founded.\n","    Many students survived by eating these noodles, but they don't even know him.\"\"\""]},{"cell_type":"code","execution_count":null,"id":"868a3dee-8193-4ad2-b78c-0c2299428530","metadata":{"id":"868a3dee-8193-4ad2-b78c-0c2299428530","outputId":"cfdcbc9d-d8bb-4b4e-b458-0a86f2790a90"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package omw-1.4 to /home/wissem/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /home/wissem/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","Some weights of the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","    Do not forget about Momofuku Ando!\n","    Momofuku Ando created instant noodles in Osaka.\n","    At Osaka, Nissin was founded.\n","    Many students survived by eating instant noodles, but Many students don't even know Nissin.\n"]}],"source":["resolved_text = coreference_resolution(text)\n","print(resolved_text)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}